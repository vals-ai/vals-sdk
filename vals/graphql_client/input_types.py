# Generated by ariadne-codegen
# Source: http://localhost:8000/graphql/

from typing import Any, List, Optional

from pydantic import Field

from .base_model import BaseModel
from .enums import RunResultSortField, RunStatus, SortOrder, TestSuiteSortField


class FilterOptionsInput(BaseModel):
    limit: Optional[int] = None
    offset: Optional[int] = None
    search: Optional[str] = None
    folder_id: Optional[str] = Field(alias="folderId", default=None)
    test_suite_id: Optional[str] = Field(alias="testSuiteId", default=None)
    sort_by: Optional[TestSuiteSortField] = Field(alias="sortBy", default=None)
    sort_order: Optional[SortOrder] = Field(alias="sortOrder", default=None)


class RunResultFilterOptionsInput(BaseModel):
    limit: Optional[int] = None
    offset: Optional[int] = None
    search: Optional[str] = None
    status: Optional[List[Optional[RunStatus]]] = None
    archived: Optional[bool] = None
    run_by: Optional[List[str]] = Field(alias="runBy", default=None)
    models_under_test: Optional[List[str]] = Field(
        alias="modelsUnderTest", default=None
    )
    suite_id: Optional[str] = Field(alias="suiteId", default=None)
    sort_by: Optional[RunResultSortField] = Field(alias="sortBy", default=None)
    sort_order: Optional[SortOrder] = Field(alias="sortOrder", default=None)


class TestMutationInfo(BaseModel):
    test_suite_id: str = Field(alias="testSuiteId")
    test_id: str = Field(alias="testId")
    input_under_test: str = Field(alias="inputUnderTest")
    checks: str
    sample_output: Optional[str] = Field(alias="sampleOutput", default=None)
    sample_output_type: Optional[str] = Field(alias="sampleOutputType", default=None)
    golden_output: Optional[str] = Field(alias="goldenOutput", default=None)
    file_ids: Optional[List[Optional[str]]] = Field(alias="fileIds", default=None)
    context: Optional[str] = None
    tags: Optional[List[Optional[str]]] = None
    file_uids: Optional[List[Optional[str]]] = Field(alias="fileUids", default=None)


class ParameterInputType(BaseModel):
    eval_model: str = Field(alias="evalModel")
    maximum_threads: int = Field(alias="maximumThreads")
    run_golden_eval: bool = Field(alias="runGoldenEval")
    run_confidence_evaluation: bool = Field(alias="runConfidenceEvaluation")
    heavyweight_factor: int = Field(alias="heavyweightFactor")
    create_text_summary: bool = Field(alias="createTextSummary")
    model_under_test: str = Field(alias="modelUnderTest")
    temperature: float
    max_output_tokens: int = Field(alias="maxOutputTokens")
    system_prompt: str = Field(alias="systemPrompt")
    new_line_stop_option: bool = Field(alias="newLineStopOption")


class PerCheckHumanReviewInputType(BaseModel):
    id: int
    binary_human_eval: Optional[int] = Field(alias="binaryHumanEval", default=None)
    is_flagged: Optional[bool] = Field(alias="isFlagged", default=None)


class CheckInputType(BaseModel):
    operator: str
    criteria: str
    modifiers: Any


class FixedOutputInputType(BaseModel):
    label: str
    text: str


class QuestionAnswerPairInputType(BaseModel):
    input_under_test: str = Field(alias="inputUnderTest")
    file_ids: Optional[List[Optional[str]]] = Field(alias="fileIds", default=None)
    context: Optional[Any] = None
    llm_output: str = Field(alias="llmOutput")
    metadata: Optional["MetadataType"] = None
    test_id: Optional[str] = Field(alias="testId", default=None)


class MetadataType(BaseModel):
    in_tokens: int = Field(alias="inTokens")
    out_tokens: int = Field(alias="outTokens")
    duration_seconds: float = Field(alias="durationSeconds")


QuestionAnswerPairInputType.model_rebuild()
